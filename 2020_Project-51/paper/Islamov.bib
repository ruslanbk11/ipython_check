% Encoding: UTF-8

@Book{Bishop2011,
  title     = {Pattern Recognition and Machine Learning},
  publisher = {SPRINGER NATURE},
  year      = {2011},
  author    = {Bishop, Christopher M.},
  isbn      = {0387310738},
  date      = {2011-04-01},
  ean       = {9780387310732},
  pagetotal = {738},
  url       = {https://www.ebook.de/de/product/5324937/christopher_m_bishop_pattern_recognition_and_machine_learning.html},
}

@Article{Yuksel2012,
  author    = {S. E. Yuksel and J. N. Wilson and P. D. Gader},
  title     = {Twenty Years of Mixture of Experts},
  journal   = {{IEEE} Transactions on Neural Networks and Learning Systems},
  year      = {2012},
  volume    = {23},
  number    = {8},
  pages     = {1177--1193},
  month     = {aug},
  doi       = {10.1109/tnnls.2012.2200299},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Rasmussen,
  author    = {Carl Edward Rasmussen and Zoubin Ghahramani},
  title     = {Infinite Mixtures of Gaussian Process Experts},
  booktitle = {In Advances in Neural Information Processing Systems 14},
  year      = {2001},
  pages     = {881--888},
  publisher = {MIT Press},
}

@Article{Jordan1994,
  author    = {Michael I. Jordan and Robert A. Jacobs},
  title     = {Hierarchical Mixtures of Experts and the {EM} Algorithm},
  journal   = {Neural Computation},
  year      = {1994},
  volume    = {6},
  number    = {2},
  pages     = {181--214},
  month     = {mar},
  doi       = {10.1162/neco.1994.6.2.181},
  publisher = {{MIT} Press - Journals},
}

@InCollection{NIPS1991_514,
  author    = {Michael I. Jordan and Robert A. Jacobs},
  title     = {Hierarchies of adaptive experts},
  booktitle = {Advances in Neural Information Processing Systems 4},
  publisher = {Morgan-Kaufmann},
  year      = {1992},
  editor    = {J. E. Moody and S. J. Hanson and R. P. Lippmann},
  pages     = {985--992},
  url       = {http://papers.nips.cc/paper/514-hierarchies-of-adaptive-experts.pdf},
}

@InCollection{Yumlu2003,
  author    = {M. Serdar Yumlu and Fikret S. Gurgen and Nesrin Okay},
  title     = {Financial Time Series Prediction Using Mixture of Experts},
  booktitle = {Computer and Information Sciences - {ISCIS} 2003},
  publisher = {Springer Berlin Heidelberg},
  year      = {2003},
  pages     = {553--560},
  doi       = {10.1007/978-3-540-39737-3_69},
}

@Article{Cheung1995,
  author = {Cheung, Yiu-ming and Leung, Wai and Xu, Lei},
  title  = {Application Of Mixture Of Experts Model To Financial Time Series Forecasting},
  year   = {1995},
  month  = oct,
}

@Article{Weigend2000,
  author    = {Andreas S. Weigend and Shanming Shi},
  title     = {Predicting daily probability distributions of S{\&}P500 returns},
  journal   = {Journal of Forecasting},
  year      = {2000},
  volume    = {19},
  number    = {4},
  pages     = {375--392},
  doi       = {10.1002/1099-131x(200007)19:4<375::aid-for779>3.0.co;2-u},
  publisher = {Wiley},
}

@Article{article,
  author  = {Ebrahimpour, Reza and Moradian, Mohammad and Esmkhani, Alireza and Jafarlou, Farzad},
  title   = {Recognition of Persian handwritten digits using Characterization Loci and Mixture of Experts.},
  journal = {JDCTA},
  year    = {2009},
  volume  = {3},
  pages   = {42-46},
  month   = jan,
}

@InProceedings{Estabrooks2001,
  author    = {Andrew Estabrooks and Nathalie Japkowicz},
  title     = {A mixture-of-experts framework for text classification},
  booktitle = {Proceedings of the 2001 workshop on Computational Natural Language Learning},
  year      = {2001},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/1117822.1117828},
}

@Article{Lima2007,
  author    = {Clodoaldo A.M. Lima and Andr{\'{e}} L.V. Coelho and Fernando J. Von Zuben},
  title     = {Hybridizing mixtures of experts with support vector machines: Investigation into nonlinear dynamic systems identification},
  journal   = {Information Sciences},
  year      = {2007},
  volume    = {177},
  number    = {10},
  pages     = {2049--2074},
  month     = {may},
  doi       = {10.1016/j.ins.2007.01.009},
  publisher = {Elsevier {BV}},
}

@Article{Cao2003,
  author    = {Lijuan Cao},
  title     = {Support vector machines experts for time series forecasting},
  journal   = {Neurocomputing},
  year      = {2003},
  volume    = {51},
  pages     = {321--339},
  month     = {apr},
  doi       = {10.1016/s0925-2312(02)00577-5},
  publisher = {Elsevier {BV}},
}

@Article{Collobert2002,
  author    = {Ronan Collobert and Samy Bengio and Yoshua Bengio},
  title     = {A Parallel Mixture of {SVMs} for Very Large Scale Problems},
  journal   = {Neural Computation},
  year      = {2002},
  volume    = {14},
  number    = {5},
  pages     = {1105--1114},
  month     = {may},
  doi       = {10.1162/089976602753633402},
  publisher = {{MIT} Press - Journals},
}

@Article{Trapp2019,
  author      = {Martin Trapp and Robert Peharz and Franz Pernkopf and Carl E. Rasmussen},
  title       = {Deep Structured Mixtures of Gaussian Processes},
  abstract    = {Gaussian Processes (GPs) are powerful non-parametric Bayesian regression models that allow exact posterior inference, but exhibit high computational and memory costs. In order to improve scalability of GPs, approximate posterior inference is frequently employed, where a prominent class of approximation techniques is based on local GP experts. However, the local-expert techniques proposed so far are either not well-principled, come with limited approximation guarantees, or lead to intractable models. In this paper, we introduce deep structured mixtures of GP experts, a stochastic process model which i) allows exact posterior inference, ii) has attractive computational and memory costs, and iii), when used as GP approximation, captures predictive uncertainties consistently better than previous approximations. In a variety of experiments, we show that deep structured mixtures have a low approximation error and outperform existing expert-based approaches.},
  date        = {2019-10-10},
  eprint      = {http://arxiv.org/abs/1910.04536v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1910.04536v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{Theis2015,
  author      = {Lucas Theis and Matthias Bethge},
  title       = {Generative Image Modeling Using Spatial LSTMs},
  abstract    = {Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
  date        = {2015-06-10},
  eprint      = {http://arxiv.org/abs/1506.03478v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1506.03478v2:PDF},
  keywords    = {stat.ML, cs.CV, cs.LG},
}

@InProceedings{Tresp01mixturesof,
  author    = {Volker Tresp},
  title     = {Mixtures of Gaussian processes},
  booktitle = {Advances in Neural Information Processing Systems 13},
  year      = {2001},
  pages     = {654--660},
  publisher = {MIT Press},
}

@Article{Shazeer2017,
  author      = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
  title       = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  abstract    = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  date        = {2017-01-23},
  eprint      = {http://arxiv.org/abs/1701.06538v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1701.06538v1:PDF},
  keywords    = {cs.LG, cs.CL, cs.NE, stat.ML},
}

@InCollection{NIPS2009_3641,
  author    = {Bangpeng Yao and Dirk Walther and Diane Beck and Li Fei-fei},
  title     = {Hierarchical Mixture of Classification Experts Uncovers Interactions between Brain Regions},
  booktitle = {Advances in Neural Information Processing Systems 22},
  publisher = {Curran Associates, Inc.},
  year      = {2009},
  editor    = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
  pages     = {2178--2186},
  url       = {http://papers.nips.cc/paper/3641-hierarchical-mixture-of-classification-experts-uncovers-interactions-between-brain-regions.pdf},
}

@Article{Aljundi2016,
  author      = {Rahaf Aljundi and Punarjay Chakravarty and Tinne Tuytelaars},
  title       = {Lifelong Learning with a Network of Experts},
  abstract    = {In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process,data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision which expert to deploy at test time. We introduce a set of gating autoencoders that learn a representation for the task at hand, and, at test time, automatically forward the test sample to the relevant expert. This also brings memory efficiency as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert, with finetuning or learning without-forgetting, can be selected. We evaluate our method on image classification and video prediction problems.},
  date        = {2016-11-18},
  eprint      = {http://arxiv.org/abs/1611.06194v2},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1611.06194v2:PDF},
  keywords    = {cs.CV, cs.AI, stat.ML},
}

@InProceedings{garmash-monz-2016-ensemble,
  author    = {Garmash, Ekaterina and Monz, Christof},
  title     = {Ensemble Learning for Multi-Source Neural Machine Translation},
  booktitle = {Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  year      = {2016},
  pages     = {1409--1418},
  address   = {Osaka, Japan},
  month     = dec,
  publisher = {The COLING 2016 Organizing Committee},
  abstract  = {In this paper we describe and evaluate methods to perform ensemble prediction in neural machine translation (NMT). We compare two methods of ensemble set induction: sampling parameter initializations for an NMT system, which is a relatively established method in NMT (Sutskever et al., 2014), and NMT systems translating from different source languages into the same target language, i.e., multi-source ensembles, a method recently introduced by Firat et al. (2016). We are motivated by the observation that for different language pairs systems make different types of mistakes. We propose several methods with different degrees of parameterization to combine individual predictions of NMT systems so that they mutually compensate for each other{'}s mistakes and improve overall performance. We find that the biggest improvements can be obtained from a context-dependent weighting scheme for multi-source ensembles. This result offers stronger support for the linguistic motivation of using multi-source ensembles than previous approaches. Evaluation is carried out for German and French into English translation. The best multi-source ensemble method achieves an improvement of up to 2.2 BLEU points over the strongest single-source ensemble baseline, and a 2 BLEU improvement over a multi-source ensemble baseline.},
  url       = {https://www.aclweb.org/anthology/C16-1133},
}

@InProceedings{Mossavat2010,
  author    = {S. Iman Mossavat and Oliver Amft and Bert de Vries and Petko N. Petkov and W. Bastiaan Kleijn},
  title     = {A bayesian hierarchical mixture of experts approach to estimate speech quality},
  booktitle = {2010 Second International Workshop on Quality of Multimedia Experience ({QoMEX})},
  year      = {2010},
  month     = {jun},
  publisher = {{IEEE}},
  doi       = {10.1109/qomex.2010.5516203},
}

@Article{Peng1996,
  author    = {Fengchun Peng and Robert A. Jacobs and Martin A. Tanner},
  title     = {Bayesian Inference in Mixtures-of-Experts and Hierarchical Mixtures-of-Experts Models with an Application to Speech Recognition},
  journal   = {Journal of the American Statistical Association},
  year      = {1996},
  volume    = {91},
  number    = {435},
  pages     = {953--960},
  month     = {sep},
  doi       = {10.1080/01621459.1996.10476965},
  publisher = {Informa {UK} Limited},
}

@Article{Ma2004,
  author    = {J.Z. Ma and L. Deng},
  title     = {Target-Directed Mixture Dynamic Models for Spontaneous Speech Recognition},
  journal   = {{IEEE} Transactions on Speech and Audio Processing},
  year      = {2004},
  volume    = {12},
  number    = {1},
  pages     = {47--58},
  month     = {jan},
  doi       = {10.1109/tsa.2003.818074},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Sminchisescu2007,
  author    = {Cristian Sminchisescu and Atul Kanaujia and Dimitris N. Metaxas},
  title     = {${BM}{^3}E$ : Discriminative Density Propagation for Visual Tracking},
  journal   = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  year      = {2007},
  volume    = {29},
  number    = {11},
  pages     = {2030--2044},
  month     = {nov},
  doi       = {10.1109/tpami.2007.1111},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Grabovoy2020,
  author  = {A. V. Grabovoy, V. V. Strijov},
  title   = {Prior distribution choices for a mixture of experts},
  journal = {Machine learning and data analysis},
  year    = {2020},
}

@Article{Islamov2020,
  author = {Islamov R.I., Grabovoy A.V.},
  title  = {https://github.com/Intelligent-Systems-Phystech/2020_Project-51/},
  year   = {2020},
  url    = {https://github.com/Intelligent-Systems-Phystech/2020_Project-51/},
}

@Comment{jabref-meta: databaseType:bibtex;}
