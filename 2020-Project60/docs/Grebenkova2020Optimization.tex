\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\bibliographystyle{jmlda-rus.bst}
\newcommand{\hdir}{.}

\begin{document}

\title
    [Вариационная оптимизация моделей глубокого обучения с контролем сложности модели] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Вариационная оптимизация моделей глубокого обучения с контролем сложности модели}
\author
    [О.\,С.~Гребенькова, О.\,Ю.~Бахтеев, В.\,В.~Стрижов] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {О.\,С.~Гребенькова, О.\,Ю.~Бахтеев, В.\,В.~Стрижов} % основной список авторов, выводимый в оглавление
    [О.\,С.~Гребенькова, О.\,Ю.~Бахтеев, В.\,В.~Стрижов] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    { grebenkova.os@phystech.edu; bakhteev@phystech.edu; strijov@ccas.ru}

\abstract
    {В работе исследуется задача построения модели глубокого обучения с возможностью задания ее сложности. Под сложностью модели понимается минимальная длина описания, минимальное количество информации, которое требуется для передачи информации о модели и о выборке. Предлагается метод оптимизации модели, основанный на представлении модели глубокого обучения в виде гиперсети с использованием байесовского подхода, где под гиперсетью понимается сеть, которая генерирует параметры другой сети. Вводятся вероятностные предположения о параметрах моделей глубокого обучения. Предлагается максимизировать вариационную нижнюю оценку байесовской обоснованности модели. Вариационная оценка рассматривается как условная величина, зависящая от задаваемой требуемой сложности модели. Для анализа качества представленного алгоритма проводятся эксперименты на выборках MNIST и CIFAR.

	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {вариационная оптимизация моделей; гиперсети; глубокое обучение; нейронные сети; байесовский подход; заданная сложность модели}
}



%данные поля заполняются редакцией журнала
%\doi{10.21469/22233792}
\receivedRus{25.02.2020}
\receivedEng{February 25, 2020}


\maketitle
\linenumbers

\section{Введение}
В данной работе рассматривается задача оптимизации модели глубокого обучения с заранее заданной сложностью модели. Построение модели заданной сложности является одной из фундаментальных проблем глубокого обучения, так как по построению данный класс моделей имеет избытычное число параметров \cite{conf/nips/Graves11}.

Предлагаемый метод заключается в представлении модели глубокого обучения в виде гиперсети, с использованием байесовского подхода. Гиперсеть --- сеть, которая задаёт параметры другой сети. В работе \cite{journals/corr/HaDL16} рассмотрены  статистические и динамические гиперсети для генерации весов сверточных и рекурентных сетей соответственно. 

Вводятся вероятностные предположения о параметрах моделей глубокого обучения. В работе \cite{conf/nips/Graves11} предлагается использовать в качестве сетевой функции ошибки минимальную длину описания, т.е. минимальное количество информации, которое требуется для передачи информации о модели и о выборке, для оптимизации параметров модели глубокого обучения. Также в работе \cite{conf/nips/Graves11} получены виды функций потерь ошибки и потерь сложности для дельта и гауссова распределений апроксимации апостериорной вероятности. 


Смежной задачей к построению модели заданной сложности выступает задача порождения и выбора оптимальной структуры сетей глубокого обучения. В работе \cite{journals/corr/SaxenaV16} рассматривается возможность порождения широкого класса свёрточных сетей как подмоделей обощенной сети, которая называется <<фабрикой>>(англ. fabric). Данные структуры позволяют обойти процесс оптимизации параметров и проверки качества одиночных сетевых архитектур. В работах \cite{journals/corr/abs-1812-09926}, \cite{journals/corr/abs-1812-03443} представлены сетевые архитектуры для решения задачи выбора структуры нейронной сети с использованием дифференцируемых алгоритмов - стохастическая (англ. Stochastic Neural Architecture Search --- SNAS) и дифференцируемая (англ. Differentiable Neural Architecture Search --- DNAS) нейронные архитектуры. Оссобенностью работы \cite{journals/corr/abs-1812-03443} является решение задачи выбора архитектуры модели, удовлетворяющей эксплутационным требованиям.

Проверка и анализ метода проводятся на выборках MNIST \cite{lecun-mnisthandwrittendigit-2010} и CIFAR-10 \cite{cif}. 

\section{Постановка задачи}

Задана выборка:
\[\mathfrak{D} = \{ \mathbf{x_{i}}, y_i\} \quad i = 1,\dots, N, \]
где $\mathbf{x_i} \in \mathbb{R}^m$, $y_i \in \{1,\dots,Y\} $, $Y$ --- число классов. Рассмотрим модель \[\mathbf{f}(\mathbf{x},\mathbf{w}):\mathbb{R}^m \times \mathbb{R}^n \longrightarrow \mathbb{R}^Y,\]
где $\mathbf{w} \in \mathbb{R}^n$ --- пространство параметров модели.


Для модели $\mathbf{f}$ и соответствующего ей вектора параметров $\mathbf{w}$ определим логарифмическую функцию правдобподобия выборки:
\begin{equation}
   \mathcal{L}_\mathfrak{D}(\mathfrak{D}, \mathbf{w}) = \log p(\mathfrak{D}|\mathbf{w}),
\end{equation}
где $p(\mathfrak{D}| \mathbf{w})$ - апостериорная вероятность выборки $\mathfrak{D}$ при заданных $\mathbf{w}$.
Оптимальные значения $\mathbf{w}$ находятся из минимизации $\mathcal{L}(\mathfrak{D})$ --- логарифма обоснованности модели:
\begin{equation}
     \log p(\mathfrak{D}) = \log \int\limits_{\mathbf{w}\in \mathbb{W}} p(\mathfrak{D}|\mathbf{w})p(\mathbf{w}) d\mathbf{w},
\end{equation}

где $p(\mathbf{w})$ --- априорная вероятность вектора параметров в пространстве $\mathbb{W}$.

Так как вычисление интеграла (2) является вычислительно сложной задачей, рассмотрим вариационный подход для решения этой задачи. Пусть задано распределение:
\[q(\mathbf{w}) \sim \mathcal{N} (\mathbf{m},\mathbf{A}_{ps}^{-1}).\]
Здесь $\mathbf{m}, \mathbf{A}_{ps}^{-1}$ --- вектор средних и матрица ковариации, аппроксимирующеее неизвестное апостериорное распределение $p(\mathbf{w}| \mathfrak{D})$, полученное при априорном предположении
\[p(\mathbf{w}) \sim \mathcal{N} (\mathbf{\mu}, \mathbf{A}_{pr}^{-1}),\]
где $\mathbf{\mu}, \mathbf{A}_{pr}^{-1}$ --- вектор средних и матрица ковариации априорного распределения. 


Приблизим интеграл (2):
\[  \mathcal{L}(\mathfrak{D}) = \log p(\mathfrak{D}) = \int\limits_{\mathbf{w}\in \mathbb{W}} q(\mathbf{w}) \log \frac{p(\mathfrak{D}, \mathbf{w})}{q(\mathbf{w})}d\mathbf{w} - \int\limits_{\mathbf{w}\in \mathbb{W}} q(\mathbf{w}) \log \frac{p(\mathbf{w}|\mathfrak{D})}{q(\mathbf{w})}d\mathbf{w} \geq \]
\[ \geq \int\limits_{\mathbf{w}\in \mathbb{W}} q(\mathbf{w}) \log \frac{p(\mathfrak{D}, \mathbf{w})}{q(\mathbf{w})}d\mathbf{w} = \int\limits_{\mathbf{w}\in \mathbb{W}} q(\mathbf{w}) \log \frac{p(\mathbf{w})}{q(\mathbf{w})}d\mathbf{w} + \int\limits_{\mathbf{w}\in \mathbb{W}} q(\mathbf{w}) \log p(\mathfrak{D}|\mathbf{w})d\mathbf{w} = \]
\begin{equation}
     = \mathcal{L}_{\mathbf{w}}(\mathfrak{D}, \mathbf{w}) + \mathcal{L}_E (\mathfrak{D})
\end{equation}


Первое слагаемое формулы (3) --- это сложность модели. Оно определяется расстоянием Кульбака–
Лейблера:
\[\mathcal{L}_{\mathbf{w}} (\mathfrak{D}, \mathbf{w}) = - D_{KL} (q(\mathbf{w})||p(\mathbf{w})).\]
Второе слагаемое формулы (3) представляет собой математическое ожидание правдоподобия выборки $\mathcal{L}_{\mathfrak{D}}(\mathfrak{D}, \mathbf{w})$.  

Обоснованность --- это один из показателей сложности модели ~\cite{conf/nips/Graves11}. В рамках данной работы рассматривается задача получения модели по обобщенной функции обоснованности:

\[ \lambda\mathcal{L}_{\mathbf{w}}(\mathfrak{D}, \mathbf{w}) + \mathcal{L}_E (\mathfrak{D}).\]

Введём множество допустимых значений параметра сложности $\varLambda \subset \mathbb{R}^+$.
Требуется найти такое отображение $\mathfrak{G}:\varLambda \longrightarrow \mathbb{W}$, чтобы для любого значения параметра сложности $\lambda \in \varLambda$  параметры доставляли бы максимум следующему функционалу:

\begin{equation}
\mathfrak{G}(\lambda) = \argmax\limits_{w \in \mathbb{W}} ( \log p(\mathfrak{D}| \mathbf{w})  + \lambda D_{KL}(q(\mathbf{w})||p(\mathbf{w})) ).
\end{equation}

%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/
\bibliography{1.bib}
\end{document}
